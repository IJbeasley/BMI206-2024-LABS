---
title: 'Lab 3: Exploratory Data Analysis'
subtitle: "BMI 206"
author: "Isobel Beasley"
date: "8th November 2024"
output: html_document
---


**Prepare the data.**

The data used in this exercise was generated by performing whole genome sequencing analysis of metastatic prostate tumors. This assay allows us to identify structural variations in tumor genomes such as duplications, inversions, and large deletions.

For more details on the analysis and results, see [Quigley et al. Cell 2018](https://pubmed.ncbi.nlm.nih.gov/30033370).

We counted the number of structural variations in each tumor sample. For our purposes, variants come in five types: inversions, deletions, duplications, insertions, and translocations. We also assessed each biopsy to identify biallelic inactivating mutations in each gene. Read in summaries of the number of structural variants present in each tumor and the presence/absence of mutations for a few selected genes.

```{r}

# Change working_folder to the path where the files are located
#working_folder = '/notebook/talks/2022_10_BMS_EDA/EDA'
fn_sum = here::here("eda/SV_summary_table.txt") #paste0( working_folder, '/SV_summary_table.txt' )
fn_mut = here::here("eda/WCDT_mutations.txt") #paste0( working_folder, '/WCDT_mutations.txt')
sv = read.table(fn_sum, sep='\t', header=TRUE, stringsAsFactors=FALSE)

mut = read.table(fn_mut, sep='\t', header=TRUE, stringsAsFactors=FALSE)

# if you don't have ggplot2 or reshape2 installed, un-comment and run the next two lines:
#install.packages("ggplot2")
#install.packages("reshape2")
library(ggplot2)
library(reshape2)
```


### Question 1: The variant showcase showdown 

The *sv* matrix object reports counts of five types of structural variants (SV) in 101 patient tumor biopsies. 

**QUESTION 1.1**: Plot a summary of the distributions for each type of SV individually. Do any of the distributions have outliers, defined as "values that exceed the whiskers of a boxplot"? 

```{r eval=TRUE, message=FALSE, warning=FALSE}
# ANSWER 1.1
library(ggplot2)
library(viridis)
library(tidyr)

sv_plot = sv |> 
          tibble::rownames_to_column("id") |>  
          tidyr::pivot_longer(data = _, 
                              cols = !id,
                            #  id_cols = "id", 
                              names_to = "group", 
                              values_to = "counts")

sv_plot |> 
  ggplot(aes(x=group, y= counts, fill = group)) + 
  geom_boxplot() + 
  theme_bw() + 
  labs(x = " ", y = "Variant Counts (per individual)") + 
  scale_fill_brewer(name = "Type of Structural Variant", palette = "Dark2")

```

**Answer (1.1):** Yes, all distributions of the counts of structural variants appear to have outliers. All of these outliers are above the mean, indicating the distributions are all right skewed. 

<br>
<br>

**QUESTION 1.2** Which SV types are most frequent? Least frequent?

```{r eval=TRUE}
# ANSWER 1.2
sort(colSums(sv), decreasing = T)
```

**Answer (1.2):** Deletions are the most frequent type of structural variant. Insertions are the least frequent type of structural variant. 

<br>
<br>

### Question 2: Is this normal?

Some analyses are contingent on distributional assumptions. For example, they may assume values are normally distributed. We can test the assumption that a sample is normally distributed with a QQ plot:

```{r}
set.seed(124)
x=rnorm(100)
qqnorm( x )
qqline( x )
```

**QUESTION 2.1** Generate a QQ plot to evaluate the assumption that deletions are normally distributed. Are deletions normally distributed?

```{r eval=TRUE}
# ANSWER 2.1
{
qqnorm(sv[, "deletions"])
qqline(sv[, "deletions"])
}
```

**Answer (2.1):** From the above plot, it does not appear that the number of deletion variants is normally distributed. The empirical quantiles of the count of deletions are not close to the expected quantiles for a normal distribution (i.e., near the line). In particular, at theoretical standard normal quantities <-1 and  >1, the points in the above Q-Q plot are much higher than the line.

<br>
<br>

### Question 3: It's not normal

Data that are not normally distributed can be coerced towards a normal distribution by transforming the data. How could you transform the distribution of deletions so that it's closer to normal? 

**QUESTION 3.1** Replot the QQ plot after performing a log transformation to see what effect your transformation had. Did this transformation make the sample more similar to a normal distribution?

**Note: leave the data as their original counts for the rest of the questions. Just transform for this question.**

```{r eval=TRUE}
# ANSWER 3.1
log_del = log(sv[, "deletions"])
{
qqnorm(log_del)
qqline(log_del)
}
```

**Answer:** Yes, the log transformation made the distribution of the number of deletion variants per individual more similar to a normal distribution. 

<br>
<br>

### Question 4: We belong together

Structural variations arise from DNA damage that is not repaired. By analyzing tumor genomes, we can figure out the kind of DNA damage that occurred by studying the patterns of SVs.

The null model is that there is no relationship between the number of any type of SV. Alternatively, there might be an association between some of the SV types, suggesting something in common about their etiology. 

**QUESTION 4.1** Calculate pairwise correlation between all five types of SV and plot the resulting correlation coefficients as a heat map. Try both Pearson correlation (the parametric default method in R) and non-parametric Spearman rank correlation, to see if it matters. 

*Hint for plotting a simple correlation heatmap, where the matrix X contains the values to plot in the heatmap:*

```{r eval=TRUE}
X = round(cor(sv), digits = 2)

XTidy = melt( X, value.name="val", varnames = c("x", "y") )

ggplot(XTidy, aes( x, y ) ) + 
     geom_tile( aes( fill = val ) ) + 
     geom_text( aes( label = val ) ) + 
  labs(x = "", y = " " , title = "Correlation between counts of different variants") + 
  scale_fill_viridis(name = "Correlation (r^2)", option = "D")
```


```{r eval=TRUE}
X = round(cor(sv, method = "spearman"), digits = 2)

XTidy = melt( X, value.name="val", varnames = c("x", "y") )

ggplot(XTidy, aes( x, y ) ) + 
     geom_tile( aes( fill = val ) ) + 
     geom_text( aes( label = val ) ) + 
    labs(x = "", y = " " , title = "Correlation between counts of different variants") + 
    scale_fill_viridis(name = "Correlation (r^2)", option = "D")
```

<br>
<br>

**QUESTION 4.2** Which types of SV are most strongly correlated with each other? Without formal testing, do the correlation data support the null model, or is there reason to investigate an alternative model? Which pairs of SV are most likely to occur in similar counts? 

```{r eval=FALSE}
# ANSWER 4.2
```

**Answer:** Duplications and inversions are the types of structural variants that most strongly correlated with each other. 

The correlation data plotted above appear to be inconsistent with the null model of there being no relationship between the number of any type of structural variants. Each category of variants is positively correlated with each other, at least weakly so (>=0.12 Pearson, >=0.23 Spearman) but in some cases moderate-strongly so (e.g., Spearman rank correlation between duplication and deletions, 0.66). Thus, especially when we consider the biological reasons to expect the number of variants of each given type to be correlated (e.g., if a DNA repair mechanism is non or less functional in a patient, then we would expect them to have increased numbers of structural mutations of many different sites), it is worth testing whether this null hypothesis truly holds. 


<br>
<br>

**QUESTION 4.3 (BONUS)**: Why are the correlation values so different when comparing Spearman rank correlation to Pearson correlation? What might be driving these differences? Does it matter?

```{r eval=FALSE}
# ANSWER 4.3
```

**Answer:** The observed correlation values are so different due to the properties of the structural variant count distributions. For instance, unlike Spearman's rank, the Pearson correlation method assumes that data is normally distributed data. However, as shown in questions 1 & 2, this assumption of normally distributed data is inconsistent with the distribution we observe for the number of deletions and, most likely, for all other structural variants. Additionally, outliers in data are likely to disproportionally impact the estimated correlation when we apply Pearson's versus Spearman's methods. Thus, the fact that we observe many outliers in the boxplots in question 1 suggests that Pearson's and Spearman's rank correlation methods would produce different results. 

Overall, the factors likely to be driving the differences between the two estimated correlations are the right skew and count-based nature of the data and the fact that some individuals have atypically large counts of some structural variants (i.e., outliers). 

As is illustrated by the above results for question 4, the difference between the two ways of calculating correlation matters not only because it changes the strength of the relationship we calculate between each kind of structural variant but because it changes the patterns of which kinds of variants are more highly associated with each other. For instance, when using Pearson correlation (plot 1), duplications are equally associated with inversions and translocations. However, when using Spearman's rank correlation (plot 2), duplications have a stronger relationship to inversions than translocations (0.66 vs 0.46), which is a difference that may lead you to draw different conclusions and follow up on other hypotheses. 

However, this difference may not matter, or at least matter less, if all the count data is log transformed to be closer to a normal distribution before these correlations are estimated. 

<br>
<br>
<br>

### Question 5: Outliers of interest

Let's drill down on two contrasting pairs of SVs:

1) duplications and inversions
2) deletions and inversions

**QUESTION 5.1** Create two scatter plots: duplications vs inversions, and deletions vs. inversions. Based on question 4, we'd expect the counts of these SVs to be somewhat correlated with each other. Does this hold up? Are there samples that are outliers from the linear trend in these comparisons?

Samples that deviate from a relationship like this might be of particular interest.

```{r eval=TRUE}
# ANSWER 5.1
sv |> 
  ggplot(aes(x=inversions, y=duplications)) + 
  geom_smooth(method = "lm", se = FALSE, formula = y~x) + 
  geom_point() + 
  theme_bw()


sv |> 
  ggplot(aes(x=inversions, y =deletions)) + 
  geom_smooth(method = "lm", se = FALSE, formula = y~x) + 
  geom_point() + 
  theme_bw()

```
**Answer:** From the above two graphs, the assumption that the numbers of mutations counted in each individual are somewhat correlated across structural variant types appears to hold.  

However, there are some outliers to this trend. In the first graph, we can see some individuals in the top left have a relatively small number of inversions (<~110) but a high number of duplication mutations (>350). Additionally, in the second graph, we can see that some individuals (top left) in this study have a high number of deletion mutations (>200) despite only having a relatively low number of inversions (<100). 

<br>
<br>
<br>

### Question 6: Why the duplicates? Why the duplicates?

Let's drill down on the duplications. Look at your plot comparing the number of duplications to the number of inversions. Note that there are three samples that have far more duplications than any other sample, and four samples that have both a large number of inversions *and* a large (though not the highest) number of duplications. Looking at this plot, we might wonder if there is something special about the three samples that have a lot of duplications but not a lot of inversions.

The *mut* matrix that you loaded contains one row for each sample and one column for each of 15 genes, with a TRUE value if there is a biallelic inactivation of that gene in that sample.

**QUESTION 6.1** We'll test the hypothesis that tumors with a particular gene inactivation acquire a lot more duplications than tumors lacking that inactivation. Test this hypothesis by performing a Wilcoxon test comparing the number of duplicates in tumors with *vs.* without mutation in each gene. Create a barplot of -log10( *p* ) and nominate the strongest hit as worthy of further investigation.

```{r eval=TRUE}
# ANSWER 6.1

test_pval = vector()

for(gene_id in 1:ncol(mut)){
  
  with_mut = mut[, gene_id]
  
  test_res =
    wilcox.test(x = sv[with_mut, "duplications"],
              y = sv[!with_mut, "duplications"],
                  alternative = "greater" )
  
  print(test_res)
  
  test_pval[gene_id] = test_res$p.value
  
}

data.frame(wilcox_pval = test_pval,
           gene = as.factor(names(mut))) |> 
  dplyr::mutate(log_pval = -log(wilcox_pval, base = 10)) |> 
  ggplot(aes(group = gene, x= gene, y=log_pval)) + 
  geom_bar(stat = "identity") + 
  labs(x = "Inactivated Gene",
       y = "-log10(Wilcox P-value)") + 
  theme_bw()

```
**Answer:** The most significant gene where gene inactivation is associated with an increased number of duplication mutations is CDK12; this strongest hit is thus worthy of further investigation. 

<br>
<br>
<br>

### Question 7: Why so many ~~deletions~~?

Now let's take a close look at deletions. Look at the scatter plot you made comparing inversions *vs* deletions. There tends to be a linear relationship, even for samples with large numbers of inversions, but there is a set of samples that have lots of deletions but not a lot of inversions. 

**QUESTION 7.1** You might hypothesize that tumors that have inactivated a DNA repair gene harbor a lot more deletions than normal tumors. Test this hypothesis by performing a Wilcox test comparing the number of deletions in tumors with *vs.* without mutation in each gene. This time, instead of a barplot, create a volcano plot to compare the difference in means (as the effect) vs. the -log10(*p*) as the statistical strength. Does this analysis nominate any gene as associated with large numbers of deletions?

```{r eval=TRUE}
# ANSWER 7.1
test_pval = vector()
test_effect_size = vector()

for(gene_id in 1:ncol(mut)){
  
  with_mut = mut[, gene_id]
  
  test_res =
    wilcox.test(x = sv[with_mut, "deletions"],
              y = sv[!with_mut, "deletions"],
                  alternative = "greater",
                   conf.int = T)
  
  print(test_res)
  
  test_pval[gene_id] = test_res$p.value
  test_effect_size[gene_id] = test_res$estimate
  
}


effect_size =  test_effect_size
logp = -log(test_pval, base = 10)

gene_names = dimnames(mut)[[2]]
plot( effect_size, logp, las=1, 
      xlim=c(-250,250), 
      ylim=c(0,7),
      xlab="change in deletions associated with mutation")
text( effect_size, logp + 0.25, gene_names ) # show gene names on the plot
```


**Answer:** Yes, the above volcano plot nominates inactivation of BRAC2 as associated with large numbers of deletions. 

<br>
<br>

### **QUESTION 7.2 (BONUS)** 

Re-run the test you performed in problem 7.1, using a *t test* rather than a Wilcoxon test. Explain why these tests have different performance and produce different results. Discuss the difference between ranking candidates based on *P* value and based on a combination of *P* value and effect size.

```{r eval=TRUE}
test_pval = vector()
test_effect_size = vector()

for(gene_id in 1:ncol(mut)){
  
  with_mut = mut[, gene_id]
  
  test_res =
    t.test(x = sv[with_mut, "deletions"],
              y = sv[!with_mut, "deletions"],
                  alternative = "greater")
  
  print(test_res)
  
  test_pval[gene_id] = test_res$p.value
  test_effect_size[gene_id] = test_res$estimate[1] - test_res$estimate[2]
  
}


effect_size =  test_effect_size
logp = -log(test_pval, base = 10)

gene_names = dimnames(mut)[[2]]
plot( effect_size, logp, las=1, 
      xlim=c(-250,250), 
      ylim=c(0,7),
      xlab="change in deletions associated with mutation")
text( effect_size, logp + 0.25, gene_names ) # show gene names on the plot
```

**Answer:** The Wilcoxon and t-tests have different performances and produce different results because the two tests have different assumptions. The t-test assumes that data is normally distributed and, in at least the version of the t-test I applied, that the variance in each group is consistent across each group. Comparatively, the Wilcoxon test does not assume normal distribution or equal variances across the two groups. When the assumptions of the t-test hold, the t-test is a more powerful method for detecting differences in means across groups, but when these assumptions do not hold, t-test results can be misleading. 

The difference between ranking candidates on p-value alone, rather than p-value and effect size, is that this can sometimes prioritize different candidates for follow-up. When we only use p-values to rank candidates, we may follow up on candidates who don't have the largest effect sizes. This scenario may occur because less variability across samples can also make p-values more significant for the same observed effect size. Thus, even though such candidates are the most statistically significant, they may not be the most biologically or clinically significant candidates because their effect may be smaller than candidates with less significant p-values but greater variability among patients. 

In our particular example of the association between gene inactivation and the number of deletions (question 7), either ranking approach prioritizes the same top hit. However, this is only sometimes the case. If we instead investigate what would be the second top hit in the plot for 7.1 for either approach, if we only consider p-value, PTEN would be the top hit. However, if we consider both p-value and effect size, we may instead prioritize CDH1 as it has a similar p-value to PTEN but a larger effect size.



